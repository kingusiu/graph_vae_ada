{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import models.graph_nn as grap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(particles):\n",
    "    idx_pt, idx_eta, idx_phi, idx_class = range(4)\n",
    "    # min-max normalize pt\n",
    "    particles[:,:,idx_pt] = (particles[:,:,idx_pt] - np.min(particles[:,:,idx_pt])) / (np.max(particles[:,:,idx_pt])-np.min(particles[:,:,idx_pt]))\n",
    "    # standard normalize angles\n",
    "    particles[:,:,idx_eta] = (particles[:,:,idx_eta] - np.mean(particles[:,:,idx_eta]))/np.std(particles[:,:,idx_eta])\n",
    "    particles[:,:,idx_eta] = (particles[:,:,idx_eta] - np.mean(particles[:,:,idx_phi]))/np.std(particles[:,:,idx_phi])\n",
    "    # min-max normalize class label\n",
    "    particles[:,:,idx_class] = (particles[:,:,idx_class] - np.min(particles[:,:,idx_class])) / (np.max(particles[:,:,idx_class])-np.min(particles[:,:,idx_class]))\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_adjacency(A):\n",
    "    D = np.array(np.sum(A, axis=2), dtype=np.float32) # compute outdegree (= rowsum)\n",
    "    D = np.nan_to_num(np.power(D,-0.5), posinf=0, neginf=0) # normalize (**-(1/2))\n",
    "    D = np.asarray([np.diagflat(dd) for dd in D]) # and diagonalize\n",
    "    return np.matmul(D, np.matmul(A, D))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_adjacencies(particles):\n",
    "    real_p_mask = particles[:,:,0] > 0\n",
    "    adjacencies = (real_p_mask[:,:,np.newaxis] * real_p_mask[:,np.newaxis,:]).astype('float32')\n",
    "    return adjacencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/kinga/dev/datasamples/L1_anomaly_challenge/background_training_500K.h5'\n",
    "ff = h5py.File(filename, 'r')\n",
    "particles = np.asarray(ff.get('Particles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 19, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_n = particles.shape[1]\n",
    "feat_sz = particles.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: array([b'Pt', b'Eta', b'Phi', b'Class'], dtype='|S5')\n",
    "batch_size = 128\n",
    "particles_train = particles[:batch_size*20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "A = make_adjacencies(particles_train)\n",
    "A_tilde = normalized_adjacency(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'pt')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_train[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_train = normalize_features(particles_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'normalized pt')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAARcUlEQVR4nO3de4yldX3H8fdHEI2g2MiaKhcHu4hutV46RU3TqtWaBVwxXnC32ordsAGLadqYul5jL1ZM2iYlRXEbca2pXLTGLmUtXiqSGlAWr1yKrriWBZEVdL0L6Ld/nAd6MuzsPLPnnDk7v32/ksme8zvPeZ7vb2b2M898n2eeJ1WFJKktD5h2AZKk8TPcJalBhrskNchwl6QGGe6S1CDDXZIaZLhL80hSSVZ2j89L8pYxr/+0JP89znVK9zp42gVIy0FVnTHtGoYleRuwsqpeOe1atH9yz11NSOKOijTEcNdUJdmR5HVJvpJkd5KLkjx46PXTk2xPcmeSLUkePfRaJfmTJF8Hvp7k2Ul2JvmLJLcn+XaSFyU5KcnXunW8cej9JyS5Msn3u2X/Kckh89S5OcnfdI8vSfKjoY9fJjmte+3xST7RbevGJKcOreMR3Rx+kOTzwK/t5fMy081vQ5Jbu/pe1722Gngj8PJu+1/ex0+/Gma4a39wKrAaOBb4DeA0gCS/B7yje/1RwLeAC+e890XA04FV3fNfBR4MHAm8Ffhn4JXAbwK/A7wlybHdsr8A/gw4Angm8FzgNQsVW1VrquqwqjoMeBlwG/CpJIcCnwA+CDwSWAu8K8m9tZ0L/Kybyx93Hwt5DnAc8Hzg9UmeV1X/CfwtcFFXx5N7rEcHGMNd+4NzqurWqroTuAR4Sjf+CuD8qvpCVf0ceAPwzCQzQ+99R1XdWVU/7Z7fDby9qu5m8IPgCOAfq+qHVXUdcD3wZICquqaqrqqqe6pqB/Ae4Fl9i07yOOD9wKlVdTPwAmBHVb2vW+cXgX8DXpbkIOAlwFur6sdVdW333oX8Zbf8V4H3Aev61qcDm31K7Q9uG3r8E+De1sujgS/c+0JV/SjJHQz2ynd0wzfPWdcdVfWL7vG9gf+dodd/ChwG94XzPwCzwEMY/H+4pk/BSQ4H/h14c1Xde8bLY4CnJ/n+0KIHAx8AVnSPh+v9Vo9NzV3+SX3qk9xz1/7sVgaBCUDX9ngEcMvQMqNc1vTdwP8Ax1XVwxj0sbPQm5I8gEHr5dNVtWnopZuBz1TVw4c+DquqM4FdwD3A0UPLH9OjxrnL39o99nKu2ivDXfuzC4BXJ3lKkgcx6DN/rmuhjMNDgR8AP0ryeODMnu97O3Ao8Kdzxv8DeFySP0zywO7jt5I8oftt4iPA25I8pOvDv6rHtt7SLf/rwKuBi7rx7wAz3Q8a6X78xtB+q6o+CbyFQd/62wzOLlk7xk28DvgD4IcMDrxetPfF77MOeAbwvaEzZl5RVT9kcOBzLYM97NuAdwIP6t53FoOW0G3AZgY99IV8BtgOfAr4u6r6eDf+oe7fO5J8YY/v1AEt3qxD2v90B42/CTywqu6Zcjlahtxzl6QGGe6S1CDbMpLUIPfcJalB+8UfMR1xxBE1MzMz7TIkaVm55pprvltVK/b02n4R7jMzM2zbtm3aZUjSspJk3r9ynmpbJsmaJJt27949zTIkqTlTDfequqSqNhx++OHTLEOSmuMBVUlqkG0ZSWqQbRlJapBtGUlqkG0ZSWqQbRlJatB+8UdMo5jZeOnUtr3j7JOntm1J2ht77pLUIHvuktQge+6S1CDbMpLUIMNdkhpkuEtSgwx3SWqQZ8tIUoM8W0aSGmRbRpIaZLhLUoMMd0lqkOEuSQ0y3CWpQZ4KKUkN8lRISWqQbRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgyYS7kkOTbItyQsmsX5J0t71Cvck5ye5Pcm1c8ZXJ7kxyfYkG4deej1w8TgLlST113fPfTOwenggyUHAucCJwCpgXZJVSX4fuB64fYx1SpIW4eA+C1XVFUlm5gyfAGyvqpsAklwInAIcBhzKIPB/mmRrVf1y7jqTbAA2ABxzzDH7PAFJ0v31Cvd5HAncPPR8J/D0qjoLIMlpwHf3FOwAVbUJ2AQwOztbI9QhSZpjlHDfq6ravNAySdYAa1auXDmpMiTpgDTK2TK3AEcPPT+qG+vNS/5K0mSMEu5XA8clOTbJIcBaYMtiVuDNOiRpMvqeCnkBcCVwfJKdSdZX1T3AWcBlwA3AxVV13WI27p67JE1G37Nl1s0zvhXYuq8bt+cuSZPhbfYkqUFeW0aSGjTVcPeAqiRNhm0ZSWqQbRlJapDhLkkNsucuSQ2y5y5JDbItI0kNMtwlqUH23CWpQfbcJalBE7tZx4FgZuOlU9nujrNPnsp2JS0f9twlqUGGuyQ1yAOqktQgD6hKUoNsy0hSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGeZ67JDXI89wlqUG2ZSSpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatDYwz3JE5Kcl+TDSc4c9/olSQvrFe5Jzk9ye5Jr54yvTnJjku1JNgJU1Q1VdQZwKvDb4y9ZkrSQvnvum4HVwwNJDgLOBU4EVgHrkqzqXnshcCmwdWyVSpJ66xXuVXUFcOec4ROA7VV1U1XdBVwInNItv6WqTgReMd86k2xIsi3Jtl27du1b9ZKkPTp4hPceCdw89Hwn8PQkzwZeDDyIvey5V9UmYBPA7OxsjVCHJGmOUcJ9j6rqcuDyPssmWQOsWbly5bjLkKQD2ihny9wCHD30/KhurDevCilJkzFKuF8NHJfk2CSHAGuBLYtZgddzl6TJ6Hsq5AXAlcDxSXYmWV9V9wBnAZcBNwAXV9V1i9m4e+6SNBm9eu5VtW6e8a14uqMk7Xe8zZ4kNcjb7ElSg7xwmCQ1yLaMJDXItowkNci2jCQ1aOyXH1gMLz+wb2Y2Xjq1be84++SpbVtSf7ZlJKlBtmUkqUGGuyQ1yFMhJalB9twlqUG2ZSSpQYa7JDXIcJekBhnuktQgz5aRpAZ5towkNci2jCQ1yHCXpAYZ7pLUIMNdkhpkuEtSgzwVUpIa5KmQktQg2zKS1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBh08iZUmeRFwMvAw4L1V9fFJbEeStGe999yTnJ/k9iTXzhlfneTGJNuTbASoqo9W1enAGcDLx1uyJGkhi2nLbAZWDw8kOQg4FzgRWAWsS7JqaJE3d69LkpZQ73CvqiuAO+cMnwBsr6qbquou4ELglAy8E/hYVX1hT+tLsiHJtiTbdu3ata/1S5L2YNQDqkcCNw8939mNvRZ4HvDSJGfs6Y1VtamqZqtqdsWKFSOWIUkaNpEDqlV1DnDOQsslWQOsWbly5STKkKQD1qh77rcARw89P6ob68VL/krSZIwa7lcDxyU5NskhwFpgS983e7MOSZqMxZwKeQFwJXB8kp1J1lfVPcBZwGXADcDFVXVd33W65y5Jk9G7515V6+YZ3wps3ZeN23NffmY2XjqV7e44++SpbFdarrzNniQ1yGvLSFKDphruHlCVpMmwLSNJDbItI0kNMtwlqUH23CWpQfbcJalBtmUkqUGGuyQ1yJ67JDXInrskNci2jCQ1yHCXpAYZ7pLUIA+oSlKDPKAqSQ2yLSNJDep9mz1pmry9n7Q47rlLUoMMd0lqkOEuSQ0y3CWpQZ7nLkkN8jx3SWqQbRlJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkho09nBP8tgk703y4XGvW5LUT69wT3J+ktuTXDtnfHWSG5NsT7IRoKpuqqr1kyhWktRP3z33zcDq4YEkBwHnAicCq4B1SVaNtTpJ0j7pFe5VdQVw55zhE4Dt3Z76XcCFwCl9N5xkQ5JtSbbt2rWrd8GSpIWN0nM/Erh56PlO4Mgkj0hyHvDUJG+Y781VtamqZqtqdsWKFSOUIUmaa+y32auqO4Az+iybZA2wZuXKleMuQxqLad3eD7zFn0Yzyp77LcDRQ8+P6sZ686qQkjQZo4T71cBxSY5NcgiwFtgynrIkSaPoeyrkBcCVwPFJdiZZX1X3AGcBlwE3ABdX1XWL2bg365CkyejVc6+qdfOMbwW27uvGq+oS4JLZ2dnT93UdkqT78zZ7ktQgb7MnSQ3ywmGS1CDbMpLUINsyktQg2zKS1CDbMpLUINsyktQg2zKS1CDDXZIaNPZL/i6Gl/yV5jetyw17qeE22HOXpAbZlpGkBhnuktQgw12SGmS4S1KD/AtVSWqQZ8tIUoNsy0hSgwx3SWqQ4S5JDTLcJalBhrskNcgLh0k64E3rIm0wuQu1eSqkJDXItowkNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQWP/C9UkhwLvAu4CLq+qfx33NiRJe9drzz3J+UluT3LtnPHVSW5Msj3Jxm74xcCHq+p04IVjrleS1EPftsxmYPXwQJKDgHOBE4FVwLokq4CjgJu7xX4xnjIlSYvRqy1TVVckmZkzfAKwvapuAkhyIXAKsJNBwH+JvfzwSLIB2ABwzDHHLLZuSRPS4kW0DkSjHFA9kv/fQ4dBqB8JfAR4SZJ3A5fM9+aq2lRVs1U1u2LFihHKkCTNNfYDqlX1Y+DVfZb1kr+SNBmj7LnfAhw99Pyobqw3L/krSZMxSrhfDRyX5NgkhwBrgS2LWUGSNUk27d69e4QyJElz9T0V8gLgSuD4JDuTrK+qe4CzgMuAG4CLq+q6xWzcPXdJmoy+Z8usm2d8K7B1rBVJkkY21csP2JaRpMnwHqqS1CD33CWpQamqaddAkl3At/bx7UcA3x1jOcuBcz4wOOcDwyhzfkxV7fGvQPeLcB9Fkm1VNTvtOpaScz4wOOcDw6Tm7PXcJalBhrskNaiFcN807QKmwDkfGJzzgWEic172PXdJ0v21sOcuSZrDcJekBi2bcJ/nfq3Drz8oyUXd65/bw52jlp0ec/7zJNcn+UqSTyV5zDTqHKeF5jy03EuSVJJlf9pcnzknObX7Wl+X5INLXeO49fjePibJp5N8sfv+PmkadY7LfPehHno9Sc7pPh9fSfK0kTdaVfv9B3AQ8A3gscAhwJeBVXOWeQ1wXvd4LXDRtOtegjk/B3hI9/jMA2HO3XIPBa4ArgJmp133EnydjwO+CPxK9/yR0657Cea8CTize7wK2DHtukec8+8CTwOunef1k4CPAQGeAXxu1G0ulz33++7XWlV3Affer3XYKcD7u8cfBp6bJEtY47gtOOeq+nRV/aR7ehWDG6YsZ32+zgB/DbwT+NlSFjchfeZ8OnBuVX0PoKpuX+Iax63PnAt4WPf4cODWJaxv7KrqCuDOvSxyCvAvNXAV8PAkjxplm8sl3Oe7X+sel6nBteZ3A49Ykuomo8+ch61n8JN/OVtwzt2vq0dX1fTu4jxefb7OjwMel+SzSa5KsnrJqpuMPnN+G/DKJDsZXFb8tUtT2tQs9v/7gsZ+D1UtvSSvBGaBZ027lklK8gDgH4DTplzKUjuYQWvm2Qx+O7siyZOq6vvTLGrC1gGbq+rvkzwT+ECSJ1bVL6dd2HKxXPbc+9yv9b5lkhzM4Fe5O5akusnodY/aJM8D3gS8sKp+vkS1TcpCc34o8ETg8iQ7GPQmtyzzg6p9vs47gS1VdXdVfRP4GoOwX676zHk9cDFAVV0JPJjBBbZaNfI9qedaLuHe536tW4BXdY9fCvxXdUcqlqkF55zkqcB7GAT7cu/DwgJzrqrdVXVEVc1U1QyD4wwvrKpt0yl3LPp8b3+UwV47SY5g0Ka5aQlrHLc+c/5f4LkASZ7AINx3LWmVS2sL8EfdWTPPAHZX1bdHWuO0jyIv4mjzSQz2WL4BvKkb+ysG/7lh8MX/ELAd+Dzw2GnXvARz/iTwHeBL3ceWadc86TnPWfZylvnZMj2/zmHQjroe+Cqwdto1L8GcVwGfZXAmzZeA50+75hHnewHwbeBuBr+JrQfOAM4Y+hqf230+vjqO72svPyBJDVoubRlJ0iIY7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalB/weCL1qZ2vfIOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_train[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('normalized pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model: Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_features (InputLa [(None, 19, 4)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_input_adjacency (InputL [(None, 19, 19)]     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution (GraphConvolu (None, 19, 3)        12          encoder_input_features[0][0]     \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_1 (GraphConvo (None, 19, 2)        6           graph_convolution[0][0]          \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "graph_convolution_2 (GraphConvo (None, 19, 1)        2           graph_convolution_1[0][0]        \n",
      "                                                                 encoder_input_adjacency[0][0]    \n",
      "==================================================================================================\n",
      "Total params: 20\n",
      "Trainable params: 20\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gnn = grap.GraphAutoencoder(nodes_n=nodes_n, feat_sz=feat_sz, activation=tf.nn.tanh)\n",
    "gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6742\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6739\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 17ms/step - val_loss: 0.6738\n",
      "Epoch 9/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00009: ReduceLROnPlateau reducing learning rate to 0.004999999888241291.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00014: ReduceLROnPlateau reducing learning rate to 0.0024999999441206455.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00019: ReduceLROnPlateau reducing learning rate to 0.0012499999720603228.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00024: ReduceLROnPlateau reducing learning rate to 0.0006249999860301614.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00029: ReduceLROnPlateau reducing learning rate to 0.0003124999930150807.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 34/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00034: ReduceLROnPlateau reducing learning rate to 0.00015624999650754035.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 39/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00039: ReduceLROnPlateau reducing learning rate to 7.812499825377017e-05.\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 44/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00044: ReduceLROnPlateau reducing learning rate to 3.9062499126885086e-05.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 11ms/step - val_loss: 0.6738\n",
      "Epoch 49/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00049: ReduceLROnPlateau reducing learning rate to 1.9531249563442543e-05.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 11ms/step - val_loss: 0.6738\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00054: ReduceLROnPlateau reducing learning rate to 9.765624781721272e-06.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 59/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00059: ReduceLROnPlateau reducing learning rate to 4.882812390860636e-06.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00064: ReduceLROnPlateau reducing learning rate to 2.441406195430318e-06.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 11ms/step - val_loss: 0.6738\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00069: ReduceLROnPlateau reducing learning rate to 1.220703097715159e-06.\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00074: ReduceLROnPlateau reducing learning rate to 6.103515488575795e-07.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 75/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 79/100\n",
      "14/15 [===========================>..] - ETA: 0s\n",
      "Epoch 00079: ReduceLROnPlateau reducing learning rate to 3.0517577442878974e-07.\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00084: ReduceLROnPlateau reducing learning rate to 1.5258788721439487e-07.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 13ms/step - val_loss: 0.6738\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 21ms/step - val_loss: 0.6738\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00089: ReduceLROnPlateau reducing learning rate to 7.629394360719743e-08.\n",
      "15/15 [==============================] - 0s 14ms/step - val_loss: 0.6738\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 12ms/step - val_loss: 0.6738\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 10ms/step - val_loss: 0.6738\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 17ms/step - val_loss: 0.6738\n",
      "Epoch 94/100\n",
      "11/15 [=====================>........] - ETA: 0s\n",
      "Epoch 00094: ReduceLROnPlateau reducing learning rate to 3.814697180359872e-08.\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 15ms/step - val_loss: 0.6738\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - ETA: 0s\n",
      "Epoch 00099: ReduceLROnPlateau reducing learning rate to 1.907348590179936e-08.\n",
      "15/15 [==============================] - 0s 19ms/step - val_loss: 0.6738\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 16ms/step - val_loss: 0.6738\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b96b9dbe0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)]\n",
    "gnn.fit((particles_train, A_tilde), A, epochs=100, batch_size=128, validation_split=0.25, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "particles_test = particles[2000:3000]\n",
    "A_test = make_adjacencies(particles_test)\n",
    "A_tilde_test = normalized_adjacency(A_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer graph_autoencoder is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z, A_pred = gnn((particles_test, A_tilde_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco = (tf.nn.sigmoid(A_pred) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_test, A_pred), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6927582"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load and predict signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/home/kinga/dev/datasamples/L1_anomaly_challenge/hToTauTau_13TeV_PU20.h5'\n",
    "ff = h5py.File(filename, 'r')\n",
    "particles_signal = np.asarray(ff.get('Particles'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(691283, 19, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "particles_signal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features: array([b'Pt', b'Eta', b'Phi', b'Class'], dtype='|S5')\n",
    "particles_signal_test = particles_signal[:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kinga/.local/lib/python3.6/site-packages/ipykernel_launcher.py:3: RuntimeWarning: divide by zero encountered in power\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "A_signal = make_adjacencies(particles_signal_test)\n",
    "A_tilde_signal = normalized_adjacency(A_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_signal_test[:,:,0].flatten())\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "particles_signal_test = normalize_features(particles_signal_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'normalized pt')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(particles_signal_test[:,:,0].flatten())\n",
    "plt.yscale('log')\n",
    "plt.title('normalized pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_signal, A_pred_signal = gnn((particles_signal_test, A_tilde_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco_signal = (tf.nn.sigmoid(A_pred_signal) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_signal, A_pred_signal), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.693138"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build model: Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnn = grap.GraphVariationalAutoencoder(nodes_n=nodes_n, feat_sz=feat_sz, activation=tf.nn.tanh)\n",
    "gnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), run_eagerly=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2560"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(particles_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "15/15 [==============================] - 0s 18ms/step - loss: 0.7996 - loss_reco: 0.7974 - loss_latent: 0.0022 - val_loss: 0.8065 - val_loss_reco: 0.8045 - val_loss_latent: 0.0020\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7990 - loss_reco: 0.7968 - loss_latent: 0.0022 - val_loss: 0.8010 - val_loss_reco: 0.7987 - val_loss_latent: 0.0022\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7983 - loss_reco: 0.7960 - loss_latent: 0.0023 - val_loss: 0.7914 - val_loss_reco: 0.7894 - val_loss_latent: 0.0020\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7988 - loss_reco: 0.7967 - loss_latent: 0.0021 - val_loss: 0.8102 - val_loss_reco: 0.8080 - val_loss_latent: 0.0022\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 16ms/step - loss: 0.7964 - loss_reco: 0.7943 - loss_latent: 0.0021 - val_loss: 0.8026 - val_loss_reco: 0.8007 - val_loss_latent: 0.0020\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 17ms/step - loss: 0.7987 - loss_reco: 0.7965 - loss_latent: 0.0022 - val_loss: 0.7933 - val_loss_reco: 0.7910 - val_loss_latent: 0.0023\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7963 - loss_reco: 0.7939 - loss_latent: 0.0024 - val_loss: 0.8044 - val_loss_reco: 0.8024 - val_loss_latent: 0.0021\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.8041 - loss_reco: 0.8019 - loss_latent: 0.0023 - val_loss: 0.8037 - val_loss_reco: 0.8013 - val_loss_latent: 0.0024\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7966 - loss_reco: 0.7944 - loss_latent: 0.0022 - val_loss: 0.8010 - val_loss_reco: 0.7990 - val_loss_latent: 0.0020\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7979 - loss_reco: 0.7957 - loss_latent: 0.0021 - val_loss: 0.7929 - val_loss_reco: 0.7907 - val_loss_latent: 0.0022\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7952 - loss_reco: 0.7931 - loss_latent: 0.0021 - val_loss: 0.7902 - val_loss_reco: 0.7882 - val_loss_latent: 0.0020\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.8009 - loss_reco: 0.7987 - loss_latent: 0.0022 - val_loss: 0.8027 - val_loss_reco: 0.8007 - val_loss_latent: 0.0020\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7988 - loss_reco: 0.7966 - loss_latent: 0.0022 - val_loss: 0.8118 - val_loss_reco: 0.8096 - val_loss_latent: 0.0022\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7987 - loss_reco: 0.7963 - loss_latent: 0.0024 - val_loss: 0.8016 - val_loss_reco: 0.7994 - val_loss_latent: 0.0022\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7994 - loss_reco: 0.7974 - loss_latent: 0.0020 - val_loss: 0.8014 - val_loss_reco: 0.7994 - val_loss_latent: 0.0020\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.7988 - loss_reco: 0.7965 - loss_latent: 0.0024 - val_loss: 0.8009 - val_loss_reco: 0.7987 - val_loss_latent: 0.0022\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7975 - loss_reco: 0.7953 - loss_latent: 0.0021 - val_loss: 0.7982 - val_loss_reco: 0.7961 - val_loss_latent: 0.0021\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7976 - loss_reco: 0.7954 - loss_latent: 0.0022 - val_loss: 0.7992 - val_loss_reco: 0.7971 - val_loss_latent: 0.0021\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7961 - loss_reco: 0.7939 - loss_latent: 0.0022 - val_loss: 0.7997 - val_loss_reco: 0.7975 - val_loss_latent: 0.0022\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7972 - loss_reco: 0.7951 - loss_latent: 0.0021 - val_loss: 0.7973 - val_loss_reco: 0.7953 - val_loss_latent: 0.0020\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 24ms/step - loss: 0.7999 - loss_reco: 0.7975 - loss_latent: 0.0024 - val_loss: 0.8016 - val_loss_reco: 0.7991 - val_loss_latent: 0.0025\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7962 - loss_reco: 0.7941 - loss_latent: 0.0022 - val_loss: 0.7974 - val_loss_reco: 0.7956 - val_loss_latent: 0.0018\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7971 - loss_reco: 0.7951 - loss_latent: 0.0020 - val_loss: 0.7927 - val_loss_reco: 0.7906 - val_loss_latent: 0.0021\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7973 - loss_reco: 0.7953 - loss_latent: 0.0021 - val_loss: 0.8037 - val_loss_reco: 0.8017 - val_loss_latent: 0.0020\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7987 - loss_reco: 0.7964 - loss_latent: 0.0022 - val_loss: 0.7911 - val_loss_reco: 0.7888 - val_loss_latent: 0.0023\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.7970 - loss_reco: 0.7949 - loss_latent: 0.0021 - val_loss: 0.7919 - val_loss_reco: 0.7900 - val_loss_latent: 0.0019\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7984 - loss_reco: 0.7961 - loss_latent: 0.0023 - val_loss: 0.8189 - val_loss_reco: 0.8167 - val_loss_latent: 0.0022\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7986 - loss_reco: 0.7965 - loss_latent: 0.0021 - val_loss: 0.8104 - val_loss_reco: 0.8085 - val_loss_latent: 0.0019\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7990 - loss_reco: 0.7968 - loss_latent: 0.0022 - val_loss: 0.8002 - val_loss_reco: 0.7981 - val_loss_latent: 0.0021\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7993 - loss_reco: 0.7972 - loss_latent: 0.0021 - val_loss: 0.7969 - val_loss_reco: 0.7948 - val_loss_latent: 0.0021\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7974 - loss_reco: 0.7954 - loss_latent: 0.0020 - val_loss: 0.8169 - val_loss_reco: 0.8149 - val_loss_latent: 0.0020\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.7952 - loss_reco: 0.7931 - loss_latent: 0.0021 - val_loss: 0.8067 - val_loss_reco: 0.8047 - val_loss_latent: 0.0021\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.7978 - loss_reco: 0.7955 - loss_latent: 0.0023 - val_loss: 0.8145 - val_loss_reco: 0.8124 - val_loss_latent: 0.0022\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.7962 - loss_reco: 0.7941 - loss_latent: 0.0021 - val_loss: 0.8029 - val_loss_reco: 0.8010 - val_loss_latent: 0.0019\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7951 - loss_reco: 0.7929 - loss_latent: 0.0022 - val_loss: 0.7980 - val_loss_reco: 0.7957 - val_loss_latent: 0.0023\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7988 - loss_reco: 0.7967 - loss_latent: 0.0021 - val_loss: 0.8040 - val_loss_reco: 0.8020 - val_loss_latent: 0.0020\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 1s 33ms/step - loss: 0.7982 - loss_reco: 0.7961 - loss_latent: 0.0021 - val_loss: 0.8115 - val_loss_reco: 0.8095 - val_loss_latent: 0.0020\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 1s 35ms/step - loss: 0.7957 - loss_reco: 0.7936 - loss_latent: 0.0021 - val_loss: 0.7987 - val_loss_reco: 0.7964 - val_loss_latent: 0.0022\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.7971 - loss_reco: 0.7949 - loss_latent: 0.0022 - val_loss: 0.7948 - val_loss_reco: 0.7927 - val_loss_latent: 0.0020\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 1s 36ms/step - loss: 0.8002 - loss_reco: 0.7980 - loss_latent: 0.0022 - val_loss: 0.8146 - val_loss_reco: 0.8123 - val_loss_latent: 0.0023\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.7988 - loss_reco: 0.7965 - loss_latent: 0.0023 - val_loss: 0.8027 - val_loss_reco: 0.8005 - val_loss_latent: 0.0022\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 33ms/step - loss: 0.7976 - loss_reco: 0.7955 - loss_latent: 0.0021 - val_loss: 0.8031 - val_loss_reco: 0.8010 - val_loss_latent: 0.0021\n",
      "Epoch 43/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 34ms/step - loss: 0.7995 - loss_reco: 0.7972 - loss_latent: 0.0022 - val_loss: 0.7955 - val_loss_reco: 0.7934 - val_loss_latent: 0.0020\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 32ms/step - loss: 0.7966 - loss_reco: 0.7944 - loss_latent: 0.0022 - val_loss: 0.7961 - val_loss_reco: 0.7940 - val_loss_latent: 0.0020\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.7951 - loss_reco: 0.7931 - loss_latent: 0.0020 - val_loss: 0.8041 - val_loss_reco: 0.8019 - val_loss_latent: 0.0021\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7981 - loss_reco: 0.7960 - loss_latent: 0.0021 - val_loss: 0.8049 - val_loss_reco: 0.8027 - val_loss_latent: 0.0021\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7937 - loss_reco: 0.7915 - loss_latent: 0.0021 - val_loss: 0.7847 - val_loss_reco: 0.7827 - val_loss_latent: 0.0020\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7986 - loss_reco: 0.7964 - loss_latent: 0.0022 - val_loss: 0.8071 - val_loss_reco: 0.8050 - val_loss_latent: 0.0021\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7981 - loss_reco: 0.7959 - loss_latent: 0.0023 - val_loss: 0.8017 - val_loss_reco: 0.7994 - val_loss_latent: 0.0022\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7987 - loss_reco: 0.7964 - loss_latent: 0.0023 - val_loss: 0.7904 - val_loss_reco: 0.7882 - val_loss_latent: 0.0021\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7997 - loss_reco: 0.7975 - loss_latent: 0.0022 - val_loss: 0.7920 - val_loss_reco: 0.7897 - val_loss_latent: 0.0023\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7979 - loss_reco: 0.7956 - loss_latent: 0.0024 - val_loss: 0.7927 - val_loss_reco: 0.7905 - val_loss_latent: 0.0022\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7966 - loss_reco: 0.7945 - loss_latent: 0.0021 - val_loss: 0.8084 - val_loss_reco: 0.8066 - val_loss_latent: 0.0018\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7985 - loss_reco: 0.7963 - loss_latent: 0.0022 - val_loss: 0.8101 - val_loss_reco: 0.8076 - val_loss_latent: 0.0025\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7960 - loss_reco: 0.7938 - loss_latent: 0.0022 - val_loss: 0.7965 - val_loss_reco: 0.7944 - val_loss_latent: 0.0020\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7977 - loss_reco: 0.7953 - loss_latent: 0.0024 - val_loss: 0.7927 - val_loss_reco: 0.7904 - val_loss_latent: 0.0023\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7960 - loss_reco: 0.7939 - loss_latent: 0.0021 - val_loss: 0.7997 - val_loss_reco: 0.7977 - val_loss_latent: 0.0021\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7962 - loss_reco: 0.7940 - loss_latent: 0.0022 - val_loss: 0.8010 - val_loss_reco: 0.7986 - val_loss_latent: 0.0024\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7975 - loss_reco: 0.7952 - loss_latent: 0.0022 - val_loss: 0.7991 - val_loss_reco: 0.7971 - val_loss_latent: 0.0020\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7970 - loss_reco: 0.7949 - loss_latent: 0.0022 - val_loss: 0.8137 - val_loss_reco: 0.8112 - val_loss_latent: 0.0025\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7969 - loss_reco: 0.7947 - loss_latent: 0.0023 - val_loss: 0.7949 - val_loss_reco: 0.7930 - val_loss_latent: 0.0019\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7993 - loss_reco: 0.7971 - loss_latent: 0.0022 - val_loss: 0.7871 - val_loss_reco: 0.7847 - val_loss_latent: 0.0023\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7975 - loss_reco: 0.7955 - loss_latent: 0.0021 - val_loss: 0.7994 - val_loss_reco: 0.7975 - val_loss_latent: 0.0019\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7989 - loss_reco: 0.7966 - loss_latent: 0.0023 - val_loss: 0.7953 - val_loss_reco: 0.7929 - val_loss_latent: 0.0024\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7986 - loss_reco: 0.7963 - loss_latent: 0.0023 - val_loss: 0.8024 - val_loss_reco: 0.8003 - val_loss_latent: 0.0021\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.8000 - loss_reco: 0.7979 - loss_latent: 0.0021 - val_loss: 0.8059 - val_loss_reco: 0.8037 - val_loss_latent: 0.0021\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7995 - loss_reco: 0.7973 - loss_latent: 0.0022 - val_loss: 0.8033 - val_loss_reco: 0.8012 - val_loss_latent: 0.0021\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7980 - loss_reco: 0.7956 - loss_latent: 0.0023 - val_loss: 0.7980 - val_loss_reco: 0.7957 - val_loss_latent: 0.0023\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 20ms/step - loss: 0.7992 - loss_reco: 0.7971 - loss_latent: 0.0022 - val_loss: 0.8001 - val_loss_reco: 0.7980 - val_loss_latent: 0.0021\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 21ms/step - loss: 0.7962 - loss_reco: 0.7940 - loss_latent: 0.0022 - val_loss: 0.8112 - val_loss_reco: 0.8091 - val_loss_latent: 0.0021\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 23ms/step - loss: 0.7965 - loss_reco: 0.7942 - loss_latent: 0.0023 - val_loss: 0.7918 - val_loss_reco: 0.7896 - val_loss_latent: 0.0022\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 22ms/step - loss: 0.7974 - loss_reco: 0.7952 - loss_latent: 0.0022 - val_loss: 0.7975 - val_loss_reco: 0.7952 - val_loss_latent: 0.0023\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7986 - loss_reco: 0.7963 - loss_latent: 0.0023 - val_loss: 0.7938 - val_loss_reco: 0.7916 - val_loss_latent: 0.0021\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 27ms/step - loss: 0.7981 - loss_reco: 0.7959 - loss_latent: 0.0022 - val_loss: 0.7966 - val_loss_reco: 0.7943 - val_loss_latent: 0.0022\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 25ms/step - loss: 0.7998 - loss_reco: 0.7975 - loss_latent: 0.0023 - val_loss: 0.7897 - val_loss_reco: 0.7873 - val_loss_latent: 0.0024\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 1s 34ms/step - loss: 0.7945 - loss_reco: 0.7924 - loss_latent: 0.0021 - val_loss: 0.8033 - val_loss_reco: 0.8014 - val_loss_latent: 0.0018\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.7994 - loss_reco: 0.7972 - loss_latent: 0.0022 - val_loss: 0.7945 - val_loss_reco: 0.7922 - val_loss_latent: 0.0023\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.7957 - loss_reco: 0.7936 - loss_latent: 0.0021 - val_loss: 0.7954 - val_loss_reco: 0.7933 - val_loss_latent: 0.0021\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 1s 37ms/step - loss: 0.7997 - loss_reco: 0.7976 - loss_latent: 0.0021 - val_loss: 0.8003 - val_loss_reco: 0.7983 - val_loss_latent: 0.0020\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.7965 - loss_reco: 0.7943 - loss_latent: 0.0022 - val_loss: 0.7948 - val_loss_reco: 0.7927 - val_loss_latent: 0.0021\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7979 - loss_reco: 0.7958 - loss_latent: 0.0022 - val_loss: 0.8024 - val_loss_reco: 0.8001 - val_loss_latent: 0.0023\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7979 - loss_reco: 0.7957 - loss_latent: 0.0023 - val_loss: 0.8002 - val_loss_reco: 0.7981 - val_loss_latent: 0.0021\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7986 - loss_reco: 0.7965 - loss_latent: 0.0020 - val_loss: 0.7998 - val_loss_reco: 0.7979 - val_loss_latent: 0.0019\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 1s 39ms/step - loss: 0.8000 - loss_reco: 0.7978 - loss_latent: 0.0022 - val_loss: 0.8109 - val_loss_reco: 0.8087 - val_loss_latent: 0.0022\n",
      "Epoch 85/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 35ms/step - loss: 0.8019 - loss_reco: 0.7996 - loss_latent: 0.0023 - val_loss: 0.8095 - val_loss_reco: 0.8071 - val_loss_latent: 0.0024\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 1s 38ms/step - loss: 0.7997 - loss_reco: 0.7974 - loss_latent: 0.0023 - val_loss: 0.8015 - val_loss_reco: 0.7992 - val_loss_latent: 0.0023\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 31ms/step - loss: 0.7984 - loss_reco: 0.7962 - loss_latent: 0.0022 - val_loss: 0.7956 - val_loss_reco: 0.7935 - val_loss_latent: 0.0022\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 26ms/step - loss: 0.7963 - loss_reco: 0.7941 - loss_latent: 0.0022 - val_loss: 0.8154 - val_loss_reco: 0.8133 - val_loss_latent: 0.0021\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7978 - loss_reco: 0.7956 - loss_latent: 0.0022 - val_loss: 0.7914 - val_loss_reco: 0.7892 - val_loss_latent: 0.0022\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7988 - loss_reco: 0.7966 - loss_latent: 0.0021 - val_loss: 0.7986 - val_loss_reco: 0.7964 - val_loss_latent: 0.0022\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7960 - loss_reco: 0.7938 - loss_latent: 0.0022 - val_loss: 0.7926 - val_loss_reco: 0.7902 - val_loss_latent: 0.0024\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7985 - loss_reco: 0.7960 - loss_latent: 0.0025 - val_loss: 0.7881 - val_loss_reco: 0.7858 - val_loss_latent: 0.0023\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7985 - loss_reco: 0.7963 - loss_latent: 0.0022 - val_loss: 0.8045 - val_loss_reco: 0.8022 - val_loss_latent: 0.0023\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 30ms/step - loss: 0.7958 - loss_reco: 0.7935 - loss_latent: 0.0023 - val_loss: 0.7923 - val_loss_reco: 0.7903 - val_loss_latent: 0.0021\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7967 - loss_reco: 0.7947 - loss_latent: 0.0021 - val_loss: 0.8015 - val_loss_reco: 0.7992 - val_loss_latent: 0.0023\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 29ms/step - loss: 0.7973 - loss_reco: 0.7952 - loss_latent: 0.0021 - val_loss: 0.8017 - val_loss_reco: 0.7999 - val_loss_latent: 0.0019\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7980 - loss_reco: 0.7959 - loss_latent: 0.0022 - val_loss: 0.7934 - val_loss_reco: 0.7909 - val_loss_latent: 0.0025\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7969 - loss_reco: 0.7947 - loss_latent: 0.0023 - val_loss: 0.8057 - val_loss_reco: 0.8035 - val_loss_latent: 0.0022\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7944 - loss_reco: 0.7922 - loss_latent: 0.0022 - val_loss: 0.8055 - val_loss_reco: 0.8034 - val_loss_latent: 0.0021\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 28ms/step - loss: 0.7965 - loss_reco: 0.7944 - loss_latent: 0.0021 - val_loss: 0.8055 - val_loss_reco: 0.8033 - val_loss_latent: 0.0022\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3b7c565e80>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [tf.keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5, verbose=1)]\n",
    "gnn.fit((particles_train, A_tilde), A, epochs=100, batch_size=batch_size, validation_split=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, z_mean, z_log_var, A_pred = gnn((particles_test, A_tilde_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco = (tf.nn.sigmoid(A_pred) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background_reco = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_test, A_pred), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_background_latent = grap.kl_loss(z_mean, z_log_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81754047"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0025711448"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_background_latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_signal, z_mean_signal, z_log_var_signal, A_pred_signal = gnn((particles_signal_test, A_tilde_signal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_reco_signal = (tf.nn.sigmoid(A_pred_signal) > 0.5).numpy().astype('int') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal_reco = tf.math.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(A_signal, A_pred_signal), axis=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_signal_latent = grap.kl_loss(z_mean_signal, z_log_var_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80608046"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal_reco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003622844"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(loss_signal_latent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
